{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../analysis/utils\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "# import drawing_utils as drawing\n",
    "import importlib\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03748a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd62a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fff9c",
   "metadata": {},
   "source": [
    "## Read dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d57bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataframes from each eventType)\n",
    "df_block = pd.read_csv(os.path.join(csv_dir,'df_block.csv'))\n",
    "df_chat = pd.read_csv(os.path.join(csv_dir,'df_chat.csv'))\n",
    "df_exit = pd.read_csv(os.path.join(csv_dir,'df_exit.csv'))\n",
    "df_trial = pd.read_csv(os.path.join(csv_dir,'df_trial.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc22018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n:', df_block.gameid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterationNames\n",
    "list(df_trial.iterationName.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92afed",
   "metadata": {},
   "source": [
    "## Exclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% Accuracy on 75% of trials\n",
    "df75 = pd.DataFrame(df_trial.groupby(['gameid', 'trialNum'])['trialScore'].sum()>75).groupby(['gameid']).sum()\n",
    "df75['trials'] = df75['trialScore']\n",
    "\n",
    "df75 = df75[df75['trials']>=9]\n",
    "includedGames = list(df75.reset_index().gameid)\n",
    "\n",
    "print(\"Total dyads achieving 75% Accuracy on 75% of trials:\",len(df75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude from analysis\n",
    "df_block = df_block[df_block.gameid.isin(includedGames)]\n",
    "df_chat = df_chat[df_chat.gameid.isin(includedGames)]\n",
    "df_exit = df_exit[df_exit.gameid.isin(includedGames)]\n",
    "df_trial = df_trial[df_trial.gameid.isin(includedGames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846f35c",
   "metadata": {},
   "source": [
    "## Task performance\n",
    "\n",
    "For accuracy, see .Rmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9d6ba",
   "metadata": {},
   "source": [
    "# Word Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc912347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps = pd.read_csv(os.path.join(csv_dir,'df_ref_exps.csv'))\n",
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps.groupby('rep')['trial_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps.loc[:,'content'] = df_ref_exps.loc[:,'content'].astype(str)\n",
    "df_ref_exps['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_ref_exps['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert number words\n",
    "\n",
    "def num_2_words(sentence):\n",
    "    out = \"\"\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            o = num2words(word)\n",
    "        except:\n",
    "            o = word\n",
    "        out = out+\" \"+ o\n",
    "    return out\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: num_2_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)]\n",
    "\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['content'].apply(lemmatize_text)\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: [i.upper() for i in x])\n",
    "\n",
    "df_ref_exps[['message','content','BOW_lemmatized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get work frequencies\n",
    "df_ref_exps['word_freq'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: Counter(x))\n",
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee319c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate lemmatized tokens, separated by spaces\n",
    "df_ref_exps['BOW_concat'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, the word counts represent the counts from all 4 of our naive raters. \n",
    "# So that we can examine how frequently different words were used, we need to convert these values into proportions.\n",
    "split_words = df_ref_exps['BOW_concat'].apply(lambda x: x.split())\n",
    "all_words = list(pd.Series([st for row in split_words for st in row]).unique())\n",
    "support = {}\n",
    "for word in all_words:\n",
    "    support[word] = 0.000000001\n",
    "    \n",
    "def get_pdist(row):\n",
    "    num_words = np.sum(list(row['word_freq'].values()))\n",
    "    pdist = support.copy()\n",
    "    for i, (word, count) in enumerate(row['word_freq'].items()):\n",
    "        pdist[word] = count/num_words\n",
    "    return pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps['word_pdist'] = df_ref_exps.apply(get_pdist, axis = 1)\n",
    "df_ref_exps['word_pdist_numeric'] = df_ref_exps['word_pdist'].apply(lambda dist: list(dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words = df_ref_exps[['dyad_gameid', 'rep', 'BOW_concat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    df_all_words.loc[:,w] = df_all_words['BOW_concat'].apply(lambda row: int(w in row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_reps = df_all_words.groupby('rep').agg(sum)\n",
    "df_all_words_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e222d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the change in word frequencies between trials.\n",
    "# prep data\n",
    "df_ref_exps_rep = df_ref_exps.groupby('rep')['BOW_concat'].apply(lambda group:' '.join(group)).reset_index()\n",
    "df_ref_exps_rep['word_freq'] = df_ref_exps_rep['BOW_concat'].apply(lambda x: Counter(x.split()))\n",
    "df_ref_exps_rep['word_pdist'] = df_ref_exps_rep.apply(get_pdist, axis=1)\n",
    "df_ref_exps_rep['word_pdist_numeric'] = df_ref_exps_rep['word_pdist'].apply(lambda dist: list(dist.values()))\n",
    "df_ref_exps_rep.index=df_ref_exps_rep['rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in proportion between reps (currently hardcoded to be 1 and 4)\n",
    "rep_a = 1 \n",
    "rep_b = 4\n",
    "\n",
    "rep_diff = {}\n",
    "\n",
    "for _, (k, rep_a_v) in enumerate(df_ref_exps_rep.loc[rep_a,'word_pdist'].items()):\n",
    "    rep_diff[k] = df_ref_exps_rep.loc[rep_b,'word_pdist'][k] - rep_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d409a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find largest n increase/ decrease in proportion across reps\n",
    "n = 6\n",
    "\n",
    "# find the largest increase in proportion between reps\n",
    "top_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "# find the largest decrease in proportion between reps\n",
    "bottom_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=False)[:n])\n",
    "\n",
    "df_grouped = df_ref_exps.groupby('rep').agg({'BOW_lemmatized': 'sum'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e12fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "font = {'fontname':'Helvetica'}\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "x_limit = 6\n",
    "\n",
    "labels, values = zip(*rep_diff.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort_high = np.argsort(values)[::-1]\n",
    "indSort_low = np.argsort(values)\n",
    "\n",
    "# rearrange your data\n",
    "#labels = np.array(labels)[indSort_high][:x_limit][::-1]\n",
    "labels = np.concatenate([np.array(labels)[indSort_low][:x_limit],np.array(labels)[indSort_high][:x_limit][::-1]])\n",
    "#values = np.array(values)[indSort_high][:x_limit][::-1]\n",
    "values = np.concatenate([np.array(values)[indSort_low][:x_limit], np.array(values)[indSort_high][:x_limit][::-1]])\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(7, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(indexes, values, color = \"#7D7D7D\")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "# add labels\n",
    "plt.yticks(fontsize=16, **font)\n",
    "plt.xticks(indexes + bar_width, labels,  rotation='vertical', fontsize=16, **font)\n",
    "plt.ylabel(\"change in proportion\", size = 24, **font)\n",
    "plt.yticks(np.arange(-.13,.06, .02))\n",
    "ax.axes.get_xaxis().set_visible(True)\n",
    "#plt.title(\"highest delta words\", size = 24, **font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d012e2",
   "metadata": {},
   "source": [
    "## Cluster analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words = df_ref_exps[['dyad_gameid', 'rep', 'BOW_concat']].copy()\n",
    "\n",
    "for w in all_words:\n",
    "    df_all_words[w] = df_all_words['BOW_concat'].apply(lambda row: int(w in row.split()))\n",
    "    \n",
    "# df_all_words_reps = df_all_words.groupby('rep').agg(sum)\n",
    "# df_all_words_reps\n",
    "# df_all_words_reps = df_all_words_reps.sort_values(by = 0, axis = 1)\n",
    "df_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae373606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count (across all four raters)\n",
    "\n",
    "df_ref_exps_trial = df_ref_exps.groupby(['dyad_gameid','rep','trial_num'])['BOW_concat'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "df_ref_exps_trial['word_freq'] = df_ref_exps_trial['BOW_concat'].apply(lambda x: Counter(x.split()))\n",
    "df_ref_exps_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61787a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_trial = df_ref_exps_trial[['dyad_gameid', 'rep', 'trial_num' ,'BOW_concat']]\n",
    "\n",
    "for w in all_words:\n",
    "    df_all_words_trial[w] = df_all_words_trial['BOW_concat'].apply(lambda row: int(w in row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48799547",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_clustering_original = AffinityPropagation(random_state=0, damping=0.5)\\\n",
    "    .fit(df_all_words_trial[df_all_words_trial.rep == 1].loc[:, 'TWO':'TWR'])\n",
    "\n",
    "r4_clustering_original = AffinityPropagation(random_state=0, damping=0.5)\\\n",
    "    .fit(df_all_words_trial[df_all_words_trial.rep == 4].loc[:,'TWO':'TWR'])\n",
    "\n",
    "df_all_words_trial.loc[(df_all_words_trial.rep == 1), 'r0_label'] = r1_clustering_original.labels_\n",
    "df_all_words_trial.loc[(df_all_words_trial.rep == 4), 'r4_label'] = r4_clustering_original.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00946cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r4_clustering_original.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b239393",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r1_clustering_original.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a815256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_trial[(df_all_words_trial.rep == 1)].loc[:,'TWO':'TWR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne, colored by clusters above\n",
    "\n",
    "tsne = TSNE(perplexity = perplexity)\n",
    "X_embedded = tsne.fit_transform(df_all_words_trial[(df_all_words_trial.rep == 1)].loc[:,'TWO':'TWR'])\n",
    "cluster_labels = r1_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "sns.scatterplot(x = X_embedded[:,0], \n",
    "                y = X_embedded[:,1], \n",
    "                hue=cluster_labels, \n",
    "                legend='full', \n",
    "                palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2223e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne, colored by clusters above\n",
    "\n",
    "tsne = TSNE(perplexity=perplexity)\n",
    "X_embedded = tsne.fit_transform(df_all_words_trial[(df_all_words_trial.rep == 4)].loc[:,'TWO':'TWR'])\n",
    "cluster_labels = r4_clustering_original.labels_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "palette = sns.color_palette(\"bright\", len(cluster_labels))\n",
    "sns.scatterplot(x = X_embedded[:,0], \n",
    "                y = X_embedded[:,1], \n",
    "                hue=cluster_labels, \n",
    "                legend='full', \n",
    "                palette='jet_r')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6802458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations using tsne, colored by clusters above (I don't know if this is a silly thing to do)\n",
    "\n",
    "np.random.seed(0)\n",
    "tsne = TSNE(perplexity=10)\n",
    "\n",
    "both_reps = pd.concat([df_all_words_trial[(df_all_words_trial.rep == 1)].loc[:,'TWO':'TWR'], df_all_words_trial[(df_all_words_trial.rep == 4)].loc[:,'TWO':'TWR']], axis=0)\n",
    "\n",
    "X_embedded = tsne.fit_transform(both_reps)\n",
    "#cluster_labels = r0_clustering_original.labels_ + r3_clustering_original.labels_\n",
    "cluster_labels = np.concatenate((r1_clustering_original.labels_, r1_clustering_original.labels_)) # visualizations of both reps are colored by their final cluster assignment, to show convergence towards strategies\n",
    "\n",
    "# r3_X_embedded = tsne.fit_transform(df_all_words_trial[(df_all_words_trial.repNum == 3)].loc[:,'two':'ablue'])\n",
    "# r3_cluster_labels = r3_clustering_original.labels_\n",
    "\n",
    "# colors = pd.concat([df_all_words_trial[(df_all_words_trial.rep == 1)], (df_all_words_trial[(df_all_words_trial.rep == 4)])], axis = 0).rep\n",
    "\n",
    "n = int(len(X_embedded[:,0])/2)\n",
    "\n",
    "# for i, x in enumerate(X_embedded[:n,0]):\n",
    "#     plt.plot([x,X_embedded[i+n,0]], [X_embedded[i,1],X_embedded[i+n,1]], color = (0,0,0,0.05) )\n",
    "\n",
    "palette = np.array(sns.color_palette(\"jet_r\", len(set(cluster_labels))))\n",
    "\n",
    "palette[[(Counter(cluster_labels[n:])[x] <= 3) for x in set(cluster_labels)]] = (0.8,0.8,0.8)\n",
    "\n",
    "palette[[(Counter(cluster_labels[n:])[x] > 3) for x in set(cluster_labels)]] = sns.color_palette(\"bright\", len(set(cluster_labels)) - sum([(Counter(cluster_labels[n:])[x] <= 3) for x in set(cluster_labels)]))\n",
    "\n",
    "palette = list(palette)\n",
    "\n",
    "palette[5] = ([0.4,0.0,0.8])\n",
    "\n",
    "# palette[6] = ([0.75,0.05,0.07])\n",
    "\n",
    "palette[7] = ([0.2,0.7,0.3])\n",
    "\n",
    "# palette[7] = ([0.2,0.7,0.3])\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x = X_embedded[:n,0], y = X_embedded[:n,1], hue=cluster_labels[:n], legend='full', palette=palette, alpha=0.8, s=160, linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    left=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "    labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "# plt.savefig('../results/plots/rep1_clusters.pdf')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# for i, x in enumerate(X_embedded[:n,0]):\n",
    "#     plt.plot([x,X_embedded[i+n,0]], [X_embedded[i,1],X_embedded[i+n,1]], color = palette[cluster_labels[i+n]], alpha=0.1)\n",
    "\n",
    "sns.scatterplot(x = X_embedded[n:,0], y = X_embedded[n:,1], hue=cluster_labels[n:], legend='full', palette=palette, alpha=0.8, s=160, linewidth=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    left=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "    labelleft=False)\n",
    "\n",
    "# plt.savefig('../results/plots/rep4_clusters.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_reps = pd.concat([df_all_words_trial[(df_all_words_trial.rep == 1)], df_all_words_trial[(df_all_words_trial.rep == 4)]], axis=0)\n",
    "df_both_reps['label'] = cluster_labels\n",
    "df_tmp = df_both_reps[df_both_reps.label == 9][['rep','BOW_concat']]\n",
    "df_tmp['BOW_concat'] =  df_tmp['BOW_concat'].apply(lambda words: pd.unique(words.split(' ')))\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3565d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_tut",
   "language": "python",
   "name": "ca_tut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
